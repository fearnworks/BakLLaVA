{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import process_images, load_image_from_base64, tokenizer_image_token, KeywordsStoppingCriteria\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "from loguru import logger \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "GB = 1 << 30\n",
    "\n",
    "worker_id = str(uuid.uuid4())[:6]\n",
    "global_counter = 0\n",
    "model_path = \"SkunkworksAI/BakLLaVA-1\"\n",
    "model_name = 'BakLLaVA-1'\n",
    "model_base = None\n",
    "\n",
    "model_semaphore = None\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path, model_base, model_name, False, False, device)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path/filename: params_dataclass.py\n",
    "from dataclasses import dataclass, fields\n",
    "from typing import Optional, List\n",
    "from queue import Queue \n",
    "from threading import Thread\n",
    "\n",
    "@dataclass\n",
    "class StreamParams:\n",
    "    \"\"\"\n",
    "    Dataclass for parameters used in generate_stream method.\n",
    "    \"\"\"\n",
    "    prompt: str\n",
    "    images: Optional[List[str]] = None\n",
    "    temperature: float = 1.0\n",
    "    top_p: float = 1.0\n",
    "    max_new_tokens: int = 256\n",
    "    stop: Optional[str] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(params_dict: dict) -> 'StreamParams':\n",
    "        \"\"\"\n",
    "        Initialize the dataclass instance from a dictionary.\n",
    "        \"\"\"\n",
    "        return StreamParams(**{f.name: params_dict.get(f.name, getattr(StreamParams, f.name)) for f in fields(StreamParams)})\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Convert the dataclass instance to a dictionary.\n",
    "        \"\"\"\n",
    "        return {f.name: getattr(self, f.name) for f in fields(StreamParams)}\n",
    "    \n",
    "# path/filename: generate_stream_functions.py\n",
    "from dataclasses import dataclass\n",
    "from typing import Generator, Any\n",
    "import torch\n",
    "import json\n",
    "from threading import Thread\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_stream(\n",
    "    params: StreamParams,\n",
    "    tokenizer: Any,\n",
    "    model: Any,\n",
    "    image_processor: Any,\n",
    "    output_queue: Queue\n",
    "):\n",
    "    try:\n",
    "        logger.info(\"Starting generate_stream function\")\n",
    "        prompt = params.prompt\n",
    "        ori_prompt = prompt\n",
    "        images = params.images or []\n",
    "        num_image_tokens = 0\n",
    "\n",
    "        # Image processing\n",
    "        if len(images) > 0:\n",
    "            logger.info(\"Processing images\")\n",
    "            if len(images) != prompt.count(DEFAULT_IMAGE_TOKEN):\n",
    "                raise ValueError(\"Number of images does not match number of <image> tokens in prompt\")\n",
    "\n",
    "            images = [load_image_from_base64(image) for image in images]\n",
    "            images = process_images(images, image_processor, model.config)\n",
    "\n",
    "            if type(images) is list:\n",
    "                images = [image.to(model.device, dtype=torch.float16) for image in images]\n",
    "            else:\n",
    "                images = images.to(model.device, dtype=torch.float16)\n",
    "\n",
    "            replace_token = DEFAULT_IMAGE_TOKEN\n",
    "            if getattr(model.config, 'mm_use_im_start_end', False):\n",
    "                replace_token = DEFAULT_IM_START_TOKEN + replace_token + DEFAULT_IM_END_TOKEN\n",
    "            prompt = prompt.replace(DEFAULT_IMAGE_TOKEN, replace_token)\n",
    "\n",
    "            num_image_tokens = prompt.count(replace_token) * model.get_vision_tower().num_patches\n",
    "        else:\n",
    "            logger.info(\"No images provided\")\n",
    "            images = None\n",
    "        image_args = {\"images\": images}\n",
    "\n",
    "        # Setting up generation parameters\n",
    "        temperature = params.temperature\n",
    "        top_p = params.top_p\n",
    "        max_context_length = getattr(model.config, 'max_position_embeddings', 2048)\n",
    "        max_new_tokens = min(params.max_new_tokens, 1024)\n",
    "        stop_str = params.stop\n",
    "        do_sample = temperature > 0.001\n",
    "        logger.info(f\"Generation parameters set : temperature={temperature}, top_p={top_p}, max_new_tokens={max_new_tokens}, stop_str={stop_str}, do_sample={do_sample}\")\n",
    "\n",
    "        # Tokenization\n",
    "        input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
    "        logger.info(\"Input tokenized and moved to device\")\n",
    "\n",
    "        # Stopping criteria\n",
    "        keywords = [stop_str] if stop_str else []\n",
    "        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "        logger.info(\"Stopping criteria established\")\n",
    "        logger.info(stopping_criteria)\n",
    "        # Text streaming setup\n",
    "        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=15)\n",
    "        logger.info(\"Text streamer initialized\")\n",
    "        logger.info(streamer)\n",
    "        # Check for token limits\n",
    "        max_new_tokens = min(max_new_tokens, max_context_length - input_ids.shape[-1] - num_image_tokens)\n",
    "        if max_new_tokens < 1:\n",
    "            logger.warning(\"Max token length exceeded\")\n",
    "            output_queue.put(json.dumps({\"text\": ori_prompt + \"Exceeds max token length. Please start a new conversation, thanks.\", \"error_code\": 0}).encode() + b\"\\0\")\n",
    "            return\n",
    "\n",
    "        # Starting generation thread\n",
    "        logger.info(\"Starting generation thread\")\n",
    "        thread = Thread(target=model.generate, kwargs=dict(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "            use_cache=True,\n",
    "            **image_args\n",
    "        ))\n",
    "        thread.start()\n",
    "\n",
    "        # Streaming generated text\n",
    "        logger.info(\"Streaming generated text\")\n",
    "        generated_text = ori_prompt\n",
    "        logger.info(generated_text)\n",
    "        for new_text in streamer:\n",
    "            logger.info(\"Text received\")\n",
    "            generated_text += new_text\n",
    "            if stop_str and generated_text.endswith(stop_str):\n",
    "                generated_text = generated_text[:-len(stop_str)]\n",
    "            output_queue.put(json.dumps({\"text\": generated_text, \"error_code\": 0}).encode() + b\"\\0\")\n",
    "\n",
    "        # Indicating end of stream\n",
    "        logger.info(\"Generation complete, ending stream\")\n",
    "        output_queue.put(None)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in generate_stream: {e}\", exc_info=True)\n",
    "        logger.error(f\"Error details: {type(e).__name__}, {e.args}\")\n",
    "        output_queue.put(json.dumps({\"error\": str(e)}).encode() + b\"\\0\")\n",
    "        output_queue.put(None)\n",
    "\n",
    "def generate_stream_gate(params: StreamParams, tokenizer, model, image_processor) -> Generator[bytes, None, None]:\n",
    "    output_queue = Queue()\n",
    "    thread = Thread(target=generate_stream, args=(params, tokenizer, model, image_processor, output_queue))\n",
    "    thread.start()\n",
    "\n",
    "    while True:\n",
    "        result = output_queue.get()\n",
    "        if result is None:  # Use a sentinel value to indicate completion\n",
    "            break\n",
    "        yield result\n",
    "\n",
    "    thread.join() \n",
    "\n",
    "from llava.conversation import default_conversation\n",
    "conv = default_conversation.copy()\n",
    "logger.info(conv)\n",
    "stop = conv.sep\n",
    "\n",
    "# Example Usage\n",
    "params_dict = {\n",
    "    \"prompt\": \"Example prompt\",\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"images\": [],\n",
    "    \"max_new_tokens\": 300\n",
    "}\n",
    "prompt = \"Caption this image\"\n",
    "params = StreamParams(prompt=prompt, stop=stop)\n",
    "\n",
    "\n",
    "\n",
    "for gen in generate_stream_gate(params, tokenizer,model, image_processor):\n",
    "    print(\"Generating\")\n",
    "    logger.info(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "\n",
    "from llava.utils import disable_torch_init\n",
    "\n",
    "\n",
    "# new stopping implementation\n",
    "class KeywordsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, keywords, tokenizer, input_ids):\n",
    "        self.keywords = keywords\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_len = None\n",
    "        self.input_ids = input_ids\n",
    "\n",
    "    def __call__(\n",
    "        self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs\n",
    "    ) -> bool:\n",
    "        if self.start_len is None:\n",
    "            self.start_len = self.input_ids.shape[1]\n",
    "        else:\n",
    "            outputs = self.tokenizer.batch_decode(\n",
    "                output_ids[:, self.start_len :], skip_special_tokens=True\n",
    "            )[0]\n",
    "            for keyword in self.keywords:\n",
    "                if keyword in outputs:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "prompt = conv.get_prompt()\n",
    "inputs = tokenizer([prompt])\n",
    "input_ids = torch.as_tensor(inputs.input_ids).cuda()\n",
    "stopping_criteria = KeywordsStoppingCriteria([conv.sep], tokenizer, input_ids)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
